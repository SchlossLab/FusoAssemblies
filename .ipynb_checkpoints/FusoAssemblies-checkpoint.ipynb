{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Acquire data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this project I will be using data from the HMP, MetaHit, ...."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Acquire HMP data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I downloaded all of the samples from the oral sites to axiom (first just oral1, later will do all). I extracted the samples that I wanted from the metadata with the following python script and then used wget to download and tar to unzip."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%python3\n",
      "import sys\n",
      "\n",
      "meta = open('/Users/Amanda/Documents/Schloss/Fuso/Assemblies/WGS.metadata.txt', 'r')\n",
      "http = open('/Users/Amanda/Documents/Schloss/Fuso/Assemblies/WGS.https.txt', 'wt')\n",
      "\n",
      "alloral = ['attached_keratinized_gingiva', 'buccal_mucosa', 'hard_palate', 'palatine_tonsils', 'saliva', 'subgingival_plaque', 'supragingival_plaque', 'throat', 'tongue_dorsum', 'stool'] \n",
      "oral1 = ['saliva', 'tongue_dorsum', 'stool']\n",
      "\n",
      "for line in meta:\n",
      "\tline=line.strip().split('\\t')\n",
      "\tloc= line[2]\n",
      "\tsite=line[1]\n",
      "\tif line[1] in oral1:\n",
      "\t\tprint('http://downloads.hmpdacc.org', line[2], sep='', end='\\n', file=http)\n",
      "\t\t\n",
      "\n",
      "meta.close()\n",
      "http.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wget -i WGS.https.txt\n",
      "quicksubmit \"ls /mnt/EXT/Schloss-data/amanda/Fuso/WGS/*.tar.bz2 |xargs -n1 tar -jxvf\" --pm nodes=1:ppn=8,mem=48GB --jobname unzip2 --walltime 999:00:00 --cput 999:00:00"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-8-e127b431f52b>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-e127b431f52b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    wget -i WGS.https.txt\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I discovered that unzipping all of these files at once makes them much too big to save on axiom, so I'll have to unzip right before I extract the fuso reads."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CONCOCT"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Practice with test data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CONCOCT was recently published in *Nature methods* by [Alneburg et al.](http://www-ncbi-nlm-nih-gov.proxy.lib.umich.edu/pubmed/?term=binning+metagenomic+contigs+by+coverage+and+composition) They introduce a new metagenomic assembly method that bins contigs based on read coverage and kmer frequency information. \n",
      "\n",
      "In the paper they use two synthetic mock metagenomic data sets. The first called \"species\" has 101 species that were identified from HMP data using 16S. Once the organisms were chosen, they compiled all the chromosome and plasmid information from NCBI for those 101 genomes. They also created a second data set that included fewer organisms (20 organisms abundant in the gut), but these include multiple strains of E.coli, Bacteroides, and Clostridium. In both datasets the frequencies of the organisms were similar to frequencies in the HMP.\n",
      "\n",
      "From these large genome compilations, Alneburg et al. similulated reads that would be obtained from an Illumina HiSeq. The species mock has 96 samples with 7.75 million paired-end reads each. The strain mock has 64 samples with 11.75 million paired-end reads each (~2GB per sample). This is small compared to the HMP samples which can reach 69 million reads (need to look up exact range). In the simulated reads they included error profiles from a real data set. "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "working directory: /mnt/EXT/Schloss-data/amanda/Fuso/concoct/testdata/\n",
      "Data obtained from: https://export.uppmax.uu.se/b2010008/projects-public/concoct-paper-data/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, I tried to use a khmer script to interleave the paired-end reads, but it didn't work. Here's what I did."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#interleave reads for one sample\n",
      "for i in Sorted_Sample120*; do gunzip $i; done\n",
      "source activate /mnt/EXT/Schloss-data/amanda/Fuso/khmer/khmerEnv\n",
      "python2.7 /mnt/EXT/Schloss-data/amanda/Fuso/khmer/khmerEnv/bin/interleave-reads.py Sorted_Sample120_R1_seed0_CPUs42.fasta Sorted_Sample120_R2_seed0_CPUs42.fasta -o Sample120.fasta\n",
      "gzip Sample120.fasta\n",
      "#error:     \"This doesn't look like paired data! %s %s\" % (name1, name2)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead, I used the velvet script shuffleSequences_fasta to interleave all of the samples combined into one big file. This ended up taking a really long time (2 days on axiom). In the future I should find a better way to interleave paired-end read files. Maybe better not to combine into one big file?"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "cd $CONCOCT_SPECIES\n",
      "zcat Sample*R1*.fasta.gz > run1/All_R1.fa #unzips all files and cats to single file\n",
      "zcat Sample*R2*.fasta.gz > run1/All_R2.fa \n",
      "quicksubmit \"perl /mnt/EXT/Schloss-data/amanda/velvet/velvet_1.2.10/contrib/shuffleSequences_fasta/shuffleSequences_fasta.pl All_R1.fa All_R2.fa All.fa\" --pm nodes=1:ppn=8,mem=48GB --jobname shuffle5 --walltime 999:00:00 --cput 999:00:00"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Digital normalization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Digital normalization is a method created by Titas Brown which will remove highly redundant sequences to decrease the file size, remove errors, and make it easier to assemble using a single-genome assembler like velvet. There is pretty good documentation on the [read the docs](http://khmer.readthedocs.org/en/v0.5/scripts.html). I had trouble getting khmer to work with the permissions that I have on axiom, so I followed the directions to create a khmer environment. I made the alias in my bash_profile so that khmerEnv will activate the environment. This works when running jobs in the queue as well."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "quicksubmit \"khmerEnv; python2.7 /mnt/EXT/Schloss-data/amanda/Fuso/khmer/khmerEnv/bin/normalize-by-median.py -k 17 All.fa\" --pm nodes=1:ppn=8,mem=48GB --jobname DN3_nopair --walltime 999:00:00 --cput 999:00:00"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "DONE with All.fa; kept 190653 of 1488000000 or  0%\n",
      "output in All.fa.keep\n",
      "fp rate estimated to be 1.000"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There was a problem with the -p command. It said that the paired-end reads weren't in the correct format. Went ahead and ran DN without the paired option. Can later do DN again and include a cutoff of 10 or 20 to speed up the normalization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Velvet assembly"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Velvet is the assembler that Alneburg et al. used in the CONCOCT paper, so I figured I would try it the same way that they did. In the online [example](https://concoct.readthedocs.org/en/latest/complete_example.html) that includes only 4 species, they suggest to just assemble the paired end reads with a kmer of 71. This wouldn't work with my 250GB file, so I am forced to do DN first. The test data in the paper was assembled using k=41. I will assemble the normalized dataset. I would also like to be able to assemble the raw reads if I can figure out how."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "quicksubmit \"velveth velveth_k71 71 All.fa.keep\" --pm nodes=1:ppn=8,mem=48GB --jobname velveth_DN --walltime 999:00:00 --cput 999:00:00 \n",
      "quicksubmit \"velvetg velveth_k41 -cov_cutoff auto\" --pm nodes=1:ppn=8,mem=48GB --jobname velvetg_DN2 --walltime 999:00:00 --cput 999:00:00 \n",
      "\n",
      "cd $CONCOCT_SPECIES/run1/velveth_k41\n",
      "\n",
      "awk '{/>/&&++a||b+=length()}END{print b/a}' contigs.fa #find average sequence length\n",
      "292.795 bp\n",
      "grep -c '>' contigs.fa #counts number of contigs\n",
      "2556\n",
      "\n",
      "awk '!/^>/ {next} {getline s} length(s) >= 500 { print $0 \"\\n\" s }' contigs.fa > contigs.500.fa #pulls out all the contigs greater than 500bp\n",
      "\n",
      "grep -c '>' contigs.500.fa\n",
      "0 contigs greater than 500bp\n",
      "\n",
      "Log:\n",
      "Median coverage depth = 2.023810\n",
      "Final graph has 2573 nodes and n50 of 331, max 1837, total 646309, using 31194/1\n",
      "90653 reads"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This doesn't seem like a very good assembly. It's because the reads weren't paired because DN didn't like the format of the file. I did the assembly again."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Tried to run velveth again and this is the logfile:\n",
      "[0.000000] Reading FastA file All.fa.keep;\n",
      "[1.892579] 190653 sequences found\n",
      "[1.892581] Done\n",
      "[2.735938] Reading read set file velveth_k71/Sequences;\n",
      "[4.064774] 190653 sequences found\n",
      "[4.299027] Done\n",
      "[4.299031] 190653 sequences in total.\n",
      "[4.310639] Writing into roadmap file velveth_k71/Roadmaps...\n",
      "[4.375395] Inputting sequences...\n",
      "[4.375399] Inputting sequence 0 / 190653\n",
      "[5.989442]  === Sequences loaded in 1.614051 s\n",
      "[6.031541] Done inputting sequences\n",
      "[6.031546] Destroying splay table\n",
      "[6.049537] Splay table destroyed"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Megahit assembly"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Titas brown says that megahit is [pretty good](http://ivory.idyll.org/blog/2014-how-good-is-megahit.html) and it is faster and more memory efficient. Digital normalization is not needed. I cloned [megahit](https://github.com/voutcn/megahit) into /mnt/EXT/Schloss-data/amanda/Fuso/megahit/megahit. It's pretty simple to use. It will take fasta, fastq, or zipped files. I don't think it can take paired read information."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "quicksubmit \"python ./megahit -m 45e9 --input-cmd 'cat /mnt/EXT/Schloss-data/amanda/Fuso/concoct/testdata/species/run1/All_*' --cpu-only -l 100 -o /mnt/EXT/Schloss-data/amanda/Fuso/concoct/testdata/species/run1/megahit\" --pm nodes=1:ppn=8,mem=48GB --jobname megahit2 --walltime 999:00:00 --cput 999:00:00\n",
      "\n",
      "awk '{/>/&&++a||b+=length()}END{print b/a}' final.contigs.fa #find average sequence length\n",
      "3650.39 bp\n",
      "grep -c '>' final.contigs.fa #counts number of contigs\n",
      "100419\n",
      "\n",
      "awk '!/^>/ {next} {getline s} length(s) >= 500 { print $0 \"\\n\" s }' final.contigs.fa > final.contigs.500.fa #pulls out all the contigs greater than 500bp\n",
      "\n",
      "grep -c '>' final.contigs.500.fa\n",
      "19663 contigs greater than 500bp\n",
      "\n",
      "awk '!/^>/ {next} {getline s} length(s) >= 1000 { print $0 \"\\n\" s }' final.contigs.fa > final.contigs.1000.fa\n",
      "grep -c '>' final.contigs.1000.fa\n",
      "12741 contigs greater than 1kb\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are 12741 contigs greater than 1kb. In the CONCOCT paper they processed the contigs to filter out any less than 1kb because the composition vectors with the frequencies of each kmer isn't accurate when contigs are shorter than 1kb. This may throw away some of the rare bugs that couldn't assemble to greater than 1kb, but it will also get rid of errors from too short contigs. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "CONCOCT processing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the prelimnary assembly is done, the coverage table can be generated. First, cut up the contigs into chunks less than 10kb so that there isn't a bias towards mapping onto long contigs. Then, map the reads onto this \"reference\" of the contigs.\n",
      "\n",
      "To run concoct on axiom, I have to enter the concoct environment. I made an alias so just type \"concoctenv\"."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "cd $CONCOCT_SPECIES/run1\n",
      "python $CONCOCT/scripts/cut_up_fasta.py -c 10000 -o 0 -m megahit/final.contigs.fa > megahit/megahit.congits_c10K.fa\n",
      "bowtie2-build megahit/megahit.congits_c10K.fa megahit/megahit.congits._c10K.fa\n",
      "quicksubmit \"bowtie2-build megahit/megahit.congits_c10K.fa megahit/megahit.congits._c10K.fa\" --pm nodes=1:ppn=8,mem=48GB --jobname bowtie_megahit --walltime 999:00:00 --cput 999:00:00 \n",
      "quicksubmit \"for f in $CONCOCT_SPECIES/Sample*R1*.fasta.gz; do gzip -d $f; gzip -d $(echo $f | sed s/\"R1\"/\"R2\"/); mkdir -p map/$(basename $f .gz); cd map/$(basename $f .gz); $CONCOCT/scripts/map-bowtie2-markduplicates.sh -ct 1 -p '-f' $(echo $f | sed s/\".gz\"/\"\"/) $(echo $(echo $f | sed s/\".gz\"/\"\"/) | sed s/\"R1\"/\"R2\"/) pair $CONCOCT_SPECIES/run1/megahit/megahit.congits._c10K.fa asm_megahit bowtie2_megahit; cd ../..; done\" --pm nodes=1:ppn=8,mem=48GB --jobname markdup_megahit --walltime 999:00:00 --cput 999:00:00 --afterok '801760'\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}